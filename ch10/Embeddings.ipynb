{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
    "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
    "\n",
    "**If you like the book or the code examples here, please leave a friendly comment on [Amazon.com](https://www.amazon.com/Blueprints-Text-Analytics-Using-Python/dp/149207408X)!**\n",
    "<img src=\"../rating.png\" width=\"100\"/>\n",
    "\n",
    "\n",
    "# Chapter 10:<div class='tocSkip'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Semantic Relationships with Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark<div class='tocSkip'/>\n",
    "\n",
    "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
    "\n",
    "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
    "\n",
    "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
    "\n",
    "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup<div class='tocSkip'/>\n",
    "\n",
    "Set directory locations. If working on Google Colab: copy files and install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:23.632884Z",
     "start_time": "2021-05-08T11:23:23.604110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
    "    os.system(f'wget {GIT_ROOT}/ch10/setup.py')\n",
    "\n",
    "%run -i setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings<div class=\"tocSkip\"/>\n",
    "\n",
    "Common imports, defaults for formatting in Matplotlib, Pandas etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:25.904174Z",
     "start_time": "2021-05-08T11:23:23.635010Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# set precision for similarity values\n",
    "%precision 3 \n",
    "np.set_printoptions(suppress=True) # no scientific for small numbers\n",
    "\n",
    "# path to import blueprints packages\n",
    "sys.path.append(BASE_DIR + '/packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you will learn and what we will build\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Case for Semantic Embeddings\n",
    "## Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy Reasoning with Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "### FastText\n",
    "### Deep Contextualized Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint: Similarity Queries on Pre-Trained Models\n",
    "## Loading a Pretrained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:25.926934Z",
     "start_time": "2021-05-08T11:23:25.907312Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GENSIM_DATA_DIR'] = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:25.946235Z",
     "start_time": "2021-05-08T11:23:25.929218Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas number format\n",
    "pd.options.display.float_format = '{:.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:26.393544Z",
     "start_time": "2021-05-08T11:23:25.948239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>base_dataset</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fasttext-wiki-news-subwords-300</th>\n",
       "      <td>1005007116</td>\n",
       "      <td>Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)</td>\n",
       "      <td>{'dimension': 300}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conceptnet-numberbatch-17-06-300</th>\n",
       "      <td>1225497562</td>\n",
       "      <td>ConceptNet, word2vec, GloVe, and OpenSubtitles 2016</td>\n",
       "      <td>{'dimension': 300}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec-ruscorpora-300</th>\n",
       "      <td>208427381</td>\n",
       "      <td>Russian National Corpus (about 250M words)</td>\n",
       "      <td>{'dimension': 300, 'window_size': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec-google-news-300</th>\n",
       "      <td>1743563840</td>\n",
       "      <td>Google News (about 100 billion words)</td>\n",
       "      <td>{'dimension': 300}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove-wiki-gigaword-50</th>\n",
       "      <td>69182535</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)</td>\n",
       "      <td>{'dimension': 50}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  file_size  \\\n",
       "fasttext-wiki-news-subwords-300  1005007116   \n",
       "conceptnet-numberbatch-17-06-300 1225497562   \n",
       "word2vec-ruscorpora-300           208427381   \n",
       "word2vec-google-news-300         1743563840   \n",
       "glove-wiki-gigaword-50             69182535   \n",
       "\n",
       "                                                                                                  base_dataset  \\\n",
       "fasttext-wiki-news-subwords-300   Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)   \n",
       "conceptnet-numberbatch-17-06-300                           ConceptNet, word2vec, GloVe, and OpenSubtitles 2016   \n",
       "word2vec-ruscorpora-300                                             Russian National Corpus (about 250M words)   \n",
       "word2vec-google-news-300                                                 Google News (about 100 billion words)   \n",
       "glove-wiki-gigaword-50                                        Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)   \n",
       "\n",
       "                                                             parameters  \n",
       "fasttext-wiki-news-subwords-300                      {'dimension': 300}  \n",
       "conceptnet-numberbatch-17-06-300                     {'dimension': 300}  \n",
       "word2vec-ruscorpora-300           {'dimension': 300, 'window_size': 10}  \n",
       "word2vec-google-news-300                             {'dimension': 300}  \n",
       "glove-wiki-gigaword-50                                {'dimension': 50}  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info_df = pd.DataFrame.from_dict(api.info()['models'], orient='index')\n",
    "info_df[['file_size', 'base_dataset', 'parameters']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:26.432085Z",
     "start_time": "2021-05-08T11:23:26.395682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_records</th>\n",
       "      <th>file_size</th>\n",
       "      <th>base_dataset</th>\n",
       "      <th>reader_code</th>\n",
       "      <th>license</th>\n",
       "      <th>parameters</th>\n",
       "      <th>description</th>\n",
       "      <th>read_more</th>\n",
       "      <th>checksum</th>\n",
       "      <th>file_name</th>\n",
       "      <th>parts</th>\n",
       "      <th>preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fasttext-wiki-news-subwords-300</th>\n",
       "      <td>999999</td>\n",
       "      <td>1005007116</td>\n",
       "      <td>Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)</td>\n",
       "      <td>https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py</td>\n",
       "      <td>https://creativecommons.org/licenses/by-sa/3.0/</td>\n",
       "      <td>{'dimension': 300}</td>\n",
       "      <td>1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).</td>\n",
       "      <td>[https://fasttext.cc/docs/en/english-vectors.html, https://arxiv.org/abs/1712.09405, https://arxiv.org/abs/1607.01759]</td>\n",
       "      <td>de2bb3a20c46ce65c9c131e1ad9a77af</td>\n",
       "      <td>fasttext-wiki-news-subwords-300.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conceptnet-numberbatch-17-06-300</th>\n",
       "      <td>1917247</td>\n",
       "      <td>1225497562</td>\n",
       "      <td>ConceptNet, word2vec, GloVe, and OpenSubtitles 2016</td>\n",
       "      <td>https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py</td>\n",
       "      <td>https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt</td>\n",
       "      <td>{'dimension': 300}</td>\n",
       "      <td>ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for furth...</td>\n",
       "      <td>[http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972, https://github.com/commonsense/conceptnet-numberbatch, http://conceptnet.io/]</td>\n",
       "      <td>fd642d457adcd0ea94da0cd21b150847</td>\n",
       "      <td>conceptnet-numberbatch-17-06-300.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec-ruscorpora-300</th>\n",
       "      <td>184973</td>\n",
       "      <td>208427381</td>\n",
       "      <td>Russian National Corpus (about 250M words)</td>\n",
       "      <td>https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py</td>\n",
       "      <td>https://creativecommons.org/licenses/by/4.0/deed.en</td>\n",
       "      <td>{'dimension': 300, 'window_size': 10}</td>\n",
       "      <td>Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.</td>\n",
       "      <td>[https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models, http://rusvectores.org/en/, https://github.com/RaRe-Technologies/gensim-data/issues/3]</td>\n",
       "      <td>9bdebdc8ae6d17d20839dd9b5af10bc4</td>\n",
       "      <td>word2vec-ruscorpora-300.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>The corpus was lemmatized and tagged with Universal PoS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  num_records  file_size  \\\n",
       "fasttext-wiki-news-subwords-300        999999 1005007116   \n",
       "conceptnet-numberbatch-17-06-300      1917247 1225497562   \n",
       "word2vec-ruscorpora-300                184973  208427381   \n",
       "\n",
       "                                                                                                  base_dataset  \\\n",
       "fasttext-wiki-news-subwords-300   Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)   \n",
       "conceptnet-numberbatch-17-06-300                           ConceptNet, word2vec, GloVe, and OpenSubtitles 2016   \n",
       "word2vec-ruscorpora-300                                             Russian National Corpus (about 250M words)   \n",
       "\n",
       "                                                                                                                                      reader_code  \\\n",
       "fasttext-wiki-news-subwords-300    https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py   \n",
       "conceptnet-numberbatch-17-06-300  https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py   \n",
       "word2vec-ruscorpora-300                    https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py   \n",
       "\n",
       "                                                                                                        license  \\\n",
       "fasttext-wiki-news-subwords-300                                 https://creativecommons.org/licenses/by-sa/3.0/   \n",
       "conceptnet-numberbatch-17-06-300  https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt   \n",
       "word2vec-ruscorpora-300                                     https://creativecommons.org/licenses/by/4.0/deed.en   \n",
       "\n",
       "                                                             parameters  \\\n",
       "fasttext-wiki-news-subwords-300                      {'dimension': 300}   \n",
       "conceptnet-numberbatch-17-06-300                     {'dimension': 300}   \n",
       "word2vec-ruscorpora-300           {'dimension': 300, 'window_size': 10}   \n",
       "\n",
       "                                                                                                                                                                                                                              description  \\\n",
       "fasttext-wiki-news-subwords-300                                                                                           1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).   \n",
       "conceptnet-numberbatch-17-06-300  ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for furth...   \n",
       "word2vec-ruscorpora-300                                                                                   Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.   \n",
       "\n",
       "                                                                                                                                                                                                                               read_more  \\\n",
       "fasttext-wiki-news-subwords-300                                                                                   [https://fasttext.cc/docs/en/english-vectors.html, https://arxiv.org/abs/1712.09405, https://arxiv.org/abs/1607.01759]   \n",
       "conceptnet-numberbatch-17-06-300                                                              [http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972, https://github.com/commonsense/conceptnet-numberbatch, http://conceptnet.io/]   \n",
       "word2vec-ruscorpora-300           [https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models, http://rusvectores.org/en/, https://github.com/RaRe-Technologies/gensim-data/issues/3]   \n",
       "\n",
       "                                                          checksum  \\\n",
       "fasttext-wiki-news-subwords-300   de2bb3a20c46ce65c9c131e1ad9a77af   \n",
       "conceptnet-numberbatch-17-06-300  fd642d457adcd0ea94da0cd21b150847   \n",
       "word2vec-ruscorpora-300           9bdebdc8ae6d17d20839dd9b5af10bc4   \n",
       "\n",
       "                                                            file_name  parts  \\\n",
       "fasttext-wiki-news-subwords-300    fasttext-wiki-news-subwords-300.gz      1   \n",
       "conceptnet-numberbatch-17-06-300  conceptnet-numberbatch-17-06-300.gz      1   \n",
       "word2vec-ruscorpora-300                    word2vec-ruscorpora-300.gz      1   \n",
       "\n",
       "                                                                            preprocessing  \n",
       "fasttext-wiki-news-subwords-300                                                       NaN  \n",
       "conceptnet-numberbatch-17-06-300                                                      NaN  \n",
       "word2vec-ruscorpora-300           The corpus was lemmatized and tagged with Universal PoS  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full list of columns\n",
    "info_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:26.454682Z",
     "start_time": "2021-05-08T11:23:26.435389Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.232134Z",
     "start_time": "2021-05-08T11:23:26.457766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.261825Z",
     "start_time": "2021-05-08T11:23:47.234356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.2f'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%precision 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.289781Z",
     "start_time": "2021-05-08T11:23:47.263551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 50\n",
      "v_king  = [ 0.5   0.69 -0.6  -0.02  0.6  -0.13 -0.09  0.47 -0.62 -0.31]\n",
      "v_queen = [ 0.38  1.82 -1.26 -0.1   0.36  0.6  -0.18  0.84 -0.06 -0.76]\n",
      "similarity: 0.7839043\n"
     ]
    }
   ],
   "source": [
    "v_king = model['king']\n",
    "v_queen = model['queen']\n",
    "\n",
    "print(\"Vector size:\", model.vector_size)\n",
    "print(\"v_king  =\", v_king[:10])\n",
    "print(\"v_queen =\", v_queen[:10])\n",
    "print(\"similarity:\", model.similarity('king', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.310372Z",
     "start_time": "2021-05-08T11:23:47.291383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.3f'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.410759Z",
     "start_time": "2021-05-08T11:23:47.312082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.824), ('queen', 0.784), ('ii', 0.775)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('king', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.436281Z",
     "start_time": "2021-05-08T11:23:47.412425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.784,  0.478, -0.255], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_lion = model['lion']\n",
    "v_nano = model['nanotechnology']\n",
    "\n",
    "model.cosine_similarities(v_king, [v_queen, v_lion, v_nano])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.481778Z",
     "start_time": "2021-05-08T11:23:47.437869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.852), ('throne', 0.766), ('prince', 0.759)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.515947Z",
     "start_time": "2021-05-08T11:23:47.483439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('berlin', 0.920), ('frankfurt', 0.820), ('vienna', 0.818)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['paris', 'germany'], negative=['france'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.557183Z",
     "start_time": "2021-05-08T11:23:47.518541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paris', 0.784)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['france', 'capital'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:47.596177Z",
     "start_time": "2021-05-08T11:23:47.561084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('central', 0.797), ('western', 0.757), ('region', 0.750)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['greece', 'capital'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprints for Training and Evaluation of Your Own Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:49.507462Z",
     "start_time": "2021-05-08T11:23:47.602829Z"
    }
   },
   "outputs": [],
   "source": [
    "db_name = \"reddit-selfposts.db\"\n",
    "db_name = f\"{BASE_DIR}/data/reddit-selfposts/reddit-selfposts-ch10.db\" ### real location\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select subreddit, lemmas, text from posts_nlp\", con)\n",
    "con.close()\n",
    "\n",
    "df['lemmas'] = df['lemmas'].str.lower().str.split() # lower case tokens\n",
    "sents = df['lemmas'] # our training \"sentences\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:52.077437Z",
     "start_time": "2021-05-08T11:23:49.509653Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, npmi_scorer\n",
    "import gensim\n",
    "\n",
    "# solved compatibility issue for Gensim 4.x\n",
    "if gensim.__version__[0] > '3': # gensim 4.x string delimiter\n",
    "    delim = '-'\n",
    "else: # gensim 3.x - byte delimiter\n",
    "    delim = b'-'\n",
    "\n",
    "phrases = Phrases(sents, min_count=10, threshold=0.3, \n",
    "                  delimiter=delim, scoring=npmi_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:52.102035Z",
     "start_time": "2021-05-08T11:23:52.079296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I|had|to|replace|the|timing-belt|in|my|mercedes-c300\n"
     ]
    }
   ],
   "source": [
    "sent = \"I had to replace the timing belt in my mercedes c300\".split()\n",
    "phrased = phrases[sent]\n",
    "print('|'.join(phrased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:57.596505Z",
     "start_time": "2021-05-08T11:23:52.103909Z"
    }
   },
   "outputs": [],
   "source": [
    "# solved compatibility issue for Gensim 4.x\n",
    "if gensim.__version__[0] > '3': # gensim 4.x - find_phrases / string phrases\n",
    "\n",
    "    phrase_df = pd.DataFrame(phrases.find_phrases(sents), \n",
    "                             columns =['phrase', 'score'])\n",
    "    phrase_df = pd.DataFrame.from_dict(phrases.find_phrases(sents), orient='index').reset_index()\n",
    "    phrase_df.columns = ['phrase', 'score']\n",
    "    phrase_df = phrase_df[['phrase', 'score']].drop_duplicates() \\\n",
    "            .sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "else: # gensim 3.x - export_phrases / byte phrases\n",
    "    phrase_df = pd.DataFrame(phrases.export_phrases(sents, out_delimiter=delim), \n",
    "                             columns =['phrase', 'score'])\n",
    "    phrase_df = phrase_df[['phrase', 'score']].drop_duplicates() \\\n",
    "        .sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "    phrase_df['phrase'] = phrase_df['phrase'].map(lambda p: p.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:57.648156Z",
     "start_time": "2021-05-08T11:23:57.599903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>mercedes-c300</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             phrase  score\n",
       "83    mercedes-benz   0.80\n",
       "1416  mercedes-c300   0.47"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_df[phrase_df['phrase'].str.contains('mercedes')] .head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:23:57.727679Z",
     "start_time": "2021-05-08T11:23:57.656885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>reservation-holder</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>roadside-assistance</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre-owned</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>mud-flap</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>backup-camera</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>acura-tl</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>catalytic-converter</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sight-unseen</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>sun-visor</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>kapton-tape</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  phrase  score\n",
       "229   reservation-holder   0.71\n",
       "122  roadside-assistance   0.76\n",
       "0              pre-owned   1.25\n",
       "212             mud-flap   0.71\n",
       "60         backup-camera   0.83\n",
       "..                   ...    ...\n",
       "120             acura-tl   0.76\n",
       "24   catalytic-converter   0.92\n",
       "39          sight-unseen   0.88\n",
       "201            sun-visor   0.72\n",
       "235          kapton-tape   0.70\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some additional phrases with score > 0.7\n",
    "phrase_df.query('score > 0.7').sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:24:14.921948Z",
     "start_time": "2021-05-08T11:23:57.733437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4ecce56bff4bf6a65c64b0c339d991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.WARNING) ###\n",
    "sents = df['lemmas'] ### like above\n",
    "phrases = Phrases(sents, min_count=10, threshold=0.7, \n",
    "                  delimiter=delim, scoring=npmi_scorer)\n",
    "\n",
    "df['phrased_lemmas'] = df['lemmas'].progress_map(lambda s: phrases[s])\n",
    "sents = df['phrased_lemmas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Training Models with Gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:24:14.947220Z",
     "start_time": "2021-05-08T11:24:14.923578Z"
    }
   },
   "outputs": [],
   "source": [
    "# for Gensim training\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:24:27.190453Z",
     "start_time": "2021-05-08T11:24:14.949939Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-04 20:13:01,502: INFO: collecting all words and their counts\n",
      "2021-10-04 20:13:01,502: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-10-04 20:13:01,686: INFO: PROGRESS: at sentence #10000, processed 999568 words, keeping 27208 word types\n",
      "2021-10-04 20:13:01,887: INFO: collected 40172 word types from a corpus of 2009337 raw words and 20000 sentences\n",
      "2021-10-04 20:13:01,887: INFO: Creating a fresh vocabulary\n",
      "2021-10-04 20:13:01,971: INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 10457 unique words (26.030568555212586%% of original 40172, drops 29715)', 'datetime': '2021-10-04T20:13:01.971747', 'gensim': '4.1.2', 'python': '3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:15:42) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}\n",
      "2021-10-04 20:13:01,971: INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1965556 word corpus (97.82112209151576%% of original 2009337, drops 43781)', 'datetime': '2021-10-04T20:13:01.971747', 'gensim': '4.1.2', 'python': '3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:15:42) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}\n",
      "2021-10-04 20:13:02,052: INFO: deleting the raw counts dictionary of 40172 items\n",
      "2021-10-04 20:13:02,052: INFO: sample=0.001 downsamples 49 most-common words\n",
      "2021-10-04 20:13:02,052: INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1523772.3881868953 word corpus (77.5%% of prior 1965556)', 'datetime': '2021-10-04T20:13:02.052050', 'gensim': '4.1.2', 'python': '3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:15:42) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}\n",
      "2021-10-04 20:13:02,201: INFO: estimated required memory for 10457 words and 100 dimensions: 13594100 bytes\n",
      "2021-10-04 20:13:02,201: INFO: resetting layer weights\n",
      "2021-10-04 20:13:02,201: INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-10-04T20:13:02.201438', 'gensim': '4.1.2', 'python': '3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:15:42) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'build_vocab'}\n",
      "2021-10-04 20:13:02,201: INFO: Word2Vec lifecycle event {'msg': 'training model with 4 workers on 10457 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2021-10-04T20:13:02.201438', 'gensim': '4.1.2', 'python': '3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:15:42) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'train'}\n",
      "2021-10-04 20:13:03,230: INFO: EPOCH 1 - PROGRESS: at 46.66% examples, 702218 words/s, in_qsize 8, out_qsize 0\n",
      "2021-10-04 20:13:04,231: INFO: EPOCH 1 - PROGRESS: at 93.38% examples, 707237 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-04 20:13:04,343: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-04 20:13:04,343: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-04 20:13:04,353: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-04 20:13:04,353: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-04 20:13:04,361: INFO: EPOCH - 1 : training on 2009337 raw words (1524166 effective words) took 2.1s, 712561 effective words/s\n",
      "2021-10-04 20:13:05,369: INFO: EPOCH 2 - PROGRESS: at 48.15% examples, 729476 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-04 20:13:06,369: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-04 20:13:06,371: INFO: EPOCH 2 - PROGRESS: at 98.96% examples, 751526 words/s, in_qsize 2, out_qsize 1\n",
      "2021-10-04 20:13:06,371: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-04 20:13:06,379: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-04 20:13:06,389: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-04 20:13:06,389: INFO: EPOCH - 2 : training on 2009337 raw words (1523472 effective words) took 2.0s, 752448 effective words/s\n",
      "2021-10-04 20:13:07,410: INFO: EPOCH 3 - PROGRESS: at 50.15% examples, 749466 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-04 20:13:08,370: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-04 20:13:08,378: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-04 20:13:08,380: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-04 20:13:08,398: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-04 20:13:08,398: INFO: EPOCH - 3 : training on 2009337 raw words (1523105 effective words) took 2.0s, 760839 effective words/s\n",
      "2021-10-04 20:13:09,409: INFO: EPOCH 4 - PROGRESS: at 47.14% examples, 708352 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-04 20:13:10,419: INFO: EPOCH 4 - PROGRESS: at 98.31% examples, 742293 words/s, in_qsize 4, out_qsize 0\n",
      "2021-10-04 20:13:10,438: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-04 20:13:10,450: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-04 20:13:10,450: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-04 20:13:10,468: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-04 20:13:10,468: INFO: EPOCH - 4 : training on 2009337 raw words (1523679 effective words) took 2.1s, 736907 effective words/s\n",
      "2021-10-04 20:13:11,488: INFO: EPOCH 5 - PROGRESS: at 47.64% examples, 717920 words/s, in_qsize 8, out_qsize 0\n",
      "2021-10-04 20:13:12,490: INFO: EPOCH 5 - PROGRESS: at 87.46% examples, 661562 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-04 20:13:12,956: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-04 20:13:12,967: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-04 20:13:12,977: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-04 20:13:13,015: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-04 20:13:13,015: INFO: EPOCH - 5 : training on 2009337 raw words (1524056 effective words) took 2.5s, 600291 effective words/s\n",
      "2021-10-04 20:13:13,015: INFO: Word2Vec lifecycle event {'msg': 'training on 10046685 raw words (7618478 effective words) took 10.8s, 704906 effective words/s', 'datetime': '2021-10-04T20:13:13.015548', 'gensim': '4.1.2', 'python': '3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:15:42) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'train'}\n",
      "2021-10-04 20:13:13,015: INFO: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=10457, vector_size=100, alpha=0.025)', 'datetime': '2021-10-04T20:13:13.015548', 'gensim': '4.1.2', 'python': '3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:15:42) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sents,       # tokenized input sentences\n",
    "                 # size=100,    # size of word vectors (default 100)\n",
    "                 window=2,    # context window size (default 5)\n",
    "                 sg=1,        # use skip-gram (default 0 = CBOW)\n",
    "                 negative=5,  # number of negative samples (default 5)\n",
    "                 min_count=5, # ignore infrequent words (default 5)\n",
    "                 workers=4,)   # number of threads (default 3)\n",
    "                 # iter=5)      # number of epochs (default 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:24:27.232513Z",
     "start_time": "2021-05-08T11:24:27.193302Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:24:27.515770Z",
     "start_time": "2021-05-08T11:24:27.235795Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('./models/autos_w2v_100_2_full.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:24:27.765278Z",
     "start_time": "2021-05-08T11:24:27.518282Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./models/autos_w2v_100_2_full.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This takes several minutes to run.** Please be patient, you need this to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:11.064605Z",
     "start_time": "2021-05-08T11:24:27.767923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v\n",
      "  Variant: cbow, Window: 2, Size: 100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mW:\\workspace\\blueprints-text\\ch10\\setup.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m### to ensure repeatability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0malgo\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w2v'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "model_path = './models'\n",
    "model_prefix = 'autos'\n",
    "\n",
    "param_grid = {'w2v': {'variant': ['cbow', 'sg'], 'window': [2, 5, 30]},\n",
    "              'ft': {'variant': ['sg'], 'window': [5]}}\n",
    "size = 100\n",
    "\n",
    "for algo, params in param_grid.items(): \n",
    "    print(algo) ###\n",
    "    for variant in params['variant']:\n",
    "        sg = 1 if variant == 'sg' else 0\n",
    "        for window in params['window']:\n",
    "            print(f\"  Variant: {variant}, Window: {window}, Size: {size}\") ###\n",
    "            np.random.seed(1) ### to ensure repeatability\n",
    "            if algo == 'w2v':\n",
    "                model = Word2Vec(sents, size=size, window=window, sg=sg)\n",
    "            else:\n",
    "                model = FastText(sents, size=size, window=window, sg=sg)\n",
    "\n",
    "            file_name = f\"{model_path}/{model_prefix}_{algo}_{variant}_{window}\"\n",
    "            model.wv.save_word2vec_format(file_name + '.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Evaluating Different Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:11.525032Z",
     "start_time": "2021-05-08T11:28:11.066176Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_path = './models' ###\n",
    "\n",
    "names = ['autos_w2v_cbow_2', 'autos_w2v_sg_2', \n",
    "         'autos_w2v_sg_5', 'autos_w2v_sg_30', 'autos_ft_sg_5']\n",
    "models = {}\n",
    "\n",
    "for name in names:\n",
    "    file_name = f\"{model_path}/{name}.bin\"\n",
    "    print(f\"Loading {file_name}\") ###\n",
    "    models[name] = KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:11.554747Z",
     "start_time": "2021-05-08T11:28:11.527206Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_models(models, **kwargs):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for name, model in models:\n",
    "        df[name] = [f\"{word} {score:.3f}\" \n",
    "                    for word, score in model.most_similar(**kwargs)]\n",
    "    df.index = df.index + 1 # let row index start at 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:11.639250Z",
     "start_time": "2021-05-08T11:28:11.557001Z"
    }
   },
   "outputs": [],
   "source": [
    "compare_models([(n, models[n]) for n in names], positive='bmw', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Similar Concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy Reasoning on our own Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that your results may be slightly different to the ones printed in the book because of random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:11.784174Z",
     "start_time": "2021-05-08T11:28:11.667982Z"
    }
   },
   "outputs": [],
   "source": [
    "compare_models([(n, models[n]) for n in names], \n",
    "               positive=['f150', 'toyota'], negative=['ford'], topn=5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:11.898949Z",
     "start_time": "2021-05-08T11:28:11.789744Z"
    }
   },
   "outputs": [],
   "source": [
    "# try a different analogy\n",
    "compare_models([(n, models[n]) for n in names], \n",
    "               positive=['x3', 'audi'], negative=['bmw'], topn=5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:12.028862Z",
     "start_time": "2021-05-08T11:28:11.902265Z"
    }
   },
   "outputs": [],
   "source": [
    "# and another one\n",
    "compare_models([(n, models[n]) for n in names], \n",
    "               positive=['spark-plug'], negative=[], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprints for Visualizing Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Applying Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:43.935238Z",
     "start_time": "2021-05-08T11:28:12.041061Z"
    }
   },
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "model = models['autos_w2v_sg_30']\n",
    "words = model.vocab\n",
    "wv = [model[word] for word in words]\n",
    "\n",
    "reducer = UMAP(n_components=2, metric='cosine', n_neighbors = 15, min_dist=0.1, random_state = 12)\n",
    "reduced_wv = reducer.fit_transform(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:28:45.150651Z",
     "start_time": "2021-05-08T11:28:43.937157Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.defaults.template = \"plotly_white\" ### plotly style\n",
    "\n",
    "plot_df = pd.DataFrame.from_records(reduced_wv, columns=['x', 'y'])\n",
    "plot_df['word'] = words\n",
    "params = {'hover_data': {c: False for c in plot_df.columns}, \n",
    "          'hover_name': 'word'}\n",
    "params.update({'width': 800, 'height': 600}) ###\n",
    "\n",
    "fig = px.scatter(plot_df, x=\"x\", y=\"y\", opacity=0.3, size_max=3, **params)\n",
    "fig.update_traces(marker={'line': {'width': 0}}) ###\n",
    "fig.update_xaxes(showticklabels=False, showgrid=True, zeroline=False, visible=True) ###\n",
    "fig.update_yaxes(showticklabels=False, showgrid=True, zeroline=False, visible=True) ###\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:02.013923Z",
     "start_time": "2021-05-08T11:28:45.152371Z"
    }
   },
   "outputs": [],
   "source": [
    "from blueprints.embeddings import plot_embeddings\n",
    "\n",
    "model = models['autos_w2v_sg_30'] ###\n",
    "search = ['ford', 'lexus', 'vw', 'hyundai', \n",
    "          'goodyear', 'spark-plug', 'florida', 'navigation']\n",
    "\n",
    "_ = plot_embeddings(model, search, topn=50, show_all=True, labels=False, \n",
    "                algo='umap', n_neighbors=15, min_dist=0.1, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:05.986727Z",
     "start_time": "2021-05-08T11:29:02.015455Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models['autos_w2v_sg_30'] ###\n",
    "search = ['ford', 'bmw', 'toyota', 'tesla', 'audi', 'mercedes', 'hyundai']\n",
    "\n",
    "_ = plot_embeddings(model, search, topn=10, show_all=False, labels=True, \n",
    "    algo='umap', n_neighbors=15, min_dist=10, spread=25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:07.469126Z",
     "start_time": "2021-05-08T11:29:05.996128Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = plot_embeddings(model, search, topn=30, n_dims=3, \n",
    "    algo='umap', n_neighbors=15, min_dist=.1, spread=40, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:07.588348Z",
     "start_time": "2021-05-08T11:29:07.471413Z"
    }
   },
   "outputs": [],
   "source": [
    "# PCA plot (not in the book) - better to explain analogies:\n",
    "# difference vectors of pickup trucks \"f150\"-\"ford\", \"tacoma\"-\"toyota\" and \n",
    "# \"frontier\"-\"nissan\" are almost parallel. \n",
    "# \"x5\"-\"bmw\" is pointing to a somewhat different direction, but \"x5\" is not a pickup\n",
    "\n",
    "model = models['autos_w2v_sg_5'] \n",
    "search = ['ford', 'f150', 'toyota', 'tacoma', 'nissan', 'frontier', 'bmw', 'x5']\n",
    "_ = plot_embeddings(model, search, topn=0, algo='pca', labels=True, colors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using Tensorflow Embedding Projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:09.764687Z",
     "start_time": "2021-05-08T11:29:07.590717Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "model_path = './models' ###\n",
    "name = 'autos_w2v_sg_30'\n",
    "model = models[name]\n",
    "\n",
    "with open(f'{model_path}/{name}_words.tsv', 'w', encoding='utf-8') as tsvfile:\n",
    "    tsvfile.write('\\n'.join(model.vocab))\n",
    "\n",
    "with open(f'{model_path}/{name}_vecs.tsv', 'w', encoding='utf-8') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t', \n",
    "                        dialect=csv.unix_dialect, quoting=csv.QUOTE_MINIMAL)\n",
    "    for w in model.vocab:\n",
    "        _ = writer.writerow(model[w].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Constructing a Similarity Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:10.266833Z",
     "start_time": "2021-05-08T11:29:09.766489Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import deque\n",
    "\n",
    "def sim_tree(model, word, top_n, max_dist):\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    graph.add_node(word, dist=0)\n",
    "\n",
    "    to_visit = deque([word])\n",
    "    while len(to_visit) > 0:\n",
    "        source = to_visit.popleft() # visit next node\n",
    "        dist = graph.nodes[source]['dist']+1\n",
    "\n",
    "        if dist <= max_dist: # discover new nodes\n",
    "            for target, sim in model.most_similar(source, topn=top_n):\n",
    "                if target not in graph:\n",
    "                    to_visit.append(target)\n",
    "                    graph.add_node(target, dist=dist)\n",
    "                    graph.add_edge(source, target, sim=sim, dist=dist)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:10.312805Z",
     "start_time": "2021-05-08T11:29:10.268780Z"
    }
   },
   "outputs": [],
   "source": [
    "def plt_add_margin(pos, x_factor=0.1, y_factor=0.1):\n",
    "    # rescales the image s.t. all captions fit onto the canvas\n",
    "    x_values, y_values = zip(*pos.values())\n",
    "    x_max = max(x_values)\n",
    "    x_min = min(x_values)\n",
    "    y_max = max(y_values)\n",
    "    y_min = min(y_values)\n",
    "\n",
    "    x_margin = (x_max - x_min) * x_factor\n",
    "    y_margin = (y_max - y_min) * y_factor\n",
    "    # return (x_min - x_margin, x_max + x_margin), (y_min - y_margin, y_max + y_margin)\n",
    "\n",
    "    plt.xlim(x_min - x_margin, x_max + x_margin)\n",
    "    plt.ylim(y_min - y_margin, y_max + y_margin)\n",
    "\n",
    "def scale_weights(graph, minw=1, maxw=8):\n",
    "    # rescale similarity to interval [minw, maxw] for display\n",
    "    sims = [graph[s][t]['sim'] for (s, t) in graph.edges]\n",
    "    min_sim, max_sim = min(sims), max(sims)\n",
    "\n",
    "    for source, target in graph.edges:\n",
    "        sim = graph[source][target]['sim']\n",
    "        graph[source][target]['sim'] = (sim-min_sim)/(max_sim-min_sim)*(maxw-minw)+minw\n",
    "\n",
    "    return graph\n",
    "\n",
    "def solve_graphviz_problems(graph):\n",
    "    # Graphviz has problems with unicode\n",
    "    # this is to prevent errors during positioning\n",
    "    def clean(n):\n",
    "        n = n.replace(',', '')\n",
    "        n = n.encode().decode('ascii', errors='ignore')\n",
    "        n = re.sub(r'[{}\\[\\]]', '-', n)\n",
    "        n = re.sub(r'^\\-', '', n)\n",
    "        return n\n",
    "    \n",
    "    node_map = {n: clean(n) for n in graph.nodes}\n",
    "    # remove empty nodes\n",
    "    for n, m in node_map.items(): \n",
    "        if len(m) == 0:\n",
    "            graph.remove_node(n)\n",
    "    \n",
    "    return nx.relabel_nodes(graph, node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:10.358155Z",
     "start_time": "2021-05-08T11:29:10.314906Z"
    }
   },
   "outputs": [],
   "source": [
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "def plot_tree(graph, node_size=1000, font_size=12):\n",
    "    graph = solve_graphviz_problems(graph) ###\n",
    "\n",
    "    pos = graphviz_layout(graph, prog='twopi', root=list(graph.nodes)[0])\n",
    "    plt.figure(figsize=(10, 4), dpi=200) ###\n",
    "    plt.grid(b=None) ### hide box\n",
    "    plt.box(False) ### hide grid\n",
    "    plt_add_margin(pos) ### just for layout\n",
    "\n",
    "    colors = [graph.nodes[n]['dist'] for n in graph] # colorize by distance\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=node_size, node_color=colors, \n",
    "                           cmap='Set1', alpha=0.4)\n",
    "    nx.draw_networkx_labels(graph, pos, font_size=font_size)\n",
    "    scale_weights(graph) ### not in book\n",
    "    \n",
    "    for (n1, n2, sim) in graph.edges(data='sim'):\n",
    "         nx.draw_networkx_edges(graph, pos, [(n1, n2)], width=sim, alpha=0.2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T13:13:57.698318Z",
     "start_time": "2021-05-08T13:13:56.298902Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models['autos_w2v_sg_2']\n",
    "graph = sim_tree(model, 'noise', top_n=10, max_dist=3)\n",
    "plot_tree(graph, node_size=500, font_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:29:11.022001Z",
     "start_time": "2021-05-08T11:23:24.471Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models['autos_w2v_sg_30']\n",
    "graph = sim_tree(model, 'spark-plug', top_n=8, max_dist=2)\n",
    "plot_tree(graph, node_size=500, font_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "461px",
    "width": "526px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215.571px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
